---
title: "Codebook and cookbook for v1 (2020-2021) Spanish mobility data"

vignette: >
  %\VignetteIndexEntry{Codebook and cookbook for v1 (2020-2021) Spanish mobility data}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
execute: 
  eval: false
---

You can view this vignette any time by running `spanishoddata::spod_codebook(ver = 1)`.

The mobility data v1 (2020-2021) was originally released by the [Ministry of Transport and Sustainable Mobility (MITMA)](https://www.transportes.gob.es/). The mobility dataset is produced by [Nommon](https://www.nommon.es/){target="_blank"} using the raw data from [Orange España](https://www.orange.es/){target="_blank"}. Even though the raw data is only from one mobile phone operator, the resulting flows and and other counts of number of individuals in the data set are already resampled to be representative of the total population of Spain. The tables in the data set provide hourly flows between zones across Spain for every day of the observation period (2020-02-14 to 2021-05-09) and the number of trips per person made. This document will introduce you to the available data and provide brief code snippets on how to access it using the `spanishoddata` R package.

Key sources for this codebook include: (1) [original data collection methodology](https://cdn.mitma.gob.es/portal-web-drupal/covid-19/bigdata/mitma_-_estudio_movilidad_covid-19_informe_metodologico_v3.pdf){target="_blank"}, (2) [original data codebook in Spanish](https://opendata-movilidad.mitma.es/README%20-%20formato%20ficheros%20movilidad%20MITMA%2020201228.pdf){target="_blank"}, (3) [original data download page](https://www.transportes.gob.es/ministerio/proyectos-singulares/estudios-de-movilidad-con-big-data/estudios-de-movilidad-anteriores/covid-19/opendata-movilidad){target="_blank"}, (4) [original data license](https://opendata-movilidad.mitma.es/LICENCIA%20de%20datos%20abiertos%20del%20MITMA%2020201203.pdf){target="_blank"}, and the (5) [homepage of the open mobility data project of the Ministry of Transport and Sustainable Mobility of Spain](https://www.transportes.gob.es/ministerio/proyectos-singulares/estudio-de-movilidad-con-big-data){target="_blank"}.

*Comment: will also add links to these PDFs auto-translated to English in DeepL*

**Note: Kindly consult the documents above for any specific details on the methodology. The codebook here is only a simplified summary.**

To access any data in this codebook, please do the following:

Install `spanishoddata` R package:

```{r}
if (!require("remotes")) install.packages("remotes")
remotes::install_github("Robinlovelace/spanishoddata")
```

Set the cache folder for the package to download the files into. You may need up to 30 GB to download all data and another 30 GB if you would like to convert the downloaded data into analysis ready format (a `DuckDB` database file, or a folder of `parquet` files). You can find more info on this conversion in the [Converting the data for faster analysis](#convert-data).

```{r}
library(spanishoddata)
Sys.setenv(SPANISH_OD_DATA_DIR = "/path/to/your/cache/directory")
```


# Overall approach to working with data

The @fig-overall-flow presents the overall approach to accessing the data in the `spanishoddata` package.

![The overview of how to use the pacakge functions to get the data](media/package-functions-overview.png){#fig-overall-flow width=120% height=120%}

<!-- the svg version breaks the whole vignette, nothing renders after the image, to sort out later -->


# 1. Spatial data with zoning boundaries

The base zoning of the study ([`Distrtics`](#districts)) is made up of census districts in cities and municipalities in the rest of Spain. In areas with low populations, data from nearby areas is aggregated to comply with Data Protection regulations. Based on this base zoning, another zoning at the municipal level ([`Municipalities`](#municipalities)) is generated, including aggregations of municipalities for data protection.

## 1.1 `Districts` {#districts}

`Districts` consist of 2850 zones (compared to 10494 census districts they are based on). In urban areas you get some level of detail within cities. In rural areas, one district is often equal to a municipality, but municipalities with low population are combined into larger units to preserve privacy of individuals in the dataset.

To access it:

```{r}
districts_v1 <- spod_get_zones("dist", ver = 1)
```

The `districts_v1` object is of class `sf` consisting of polygons.

Data structure:

| Variable Name | **Description** |
|------------------------------------|------------------------------------|
| `id` | District `id` assigned by the data provider. Matches with `id_origin`, `id_destination`, and `id` in district level [origin-destination data](#od-data) and [trips per person data](#tpp-data). |
| `census_districts` | A list of census district identifiers as classified by the Spanish Statistical Office (INE) that are spatially bound within polygons with `id` above. |
| `municipalities_mitma` | A list of municipality identifiers as assigned by the data provider in municipality zones spatial dataset that correspond to a given district `id` . |
| `municipalities` | A list of municipality identifiers as classified by the Spanish Statistical Office (INE) that correspond to polygons with `id` above. |
| `district_names_in_v2` | A list of names of district polygons defined in the v2 version of this data that covers the year 2022 and onwards that correspond to polygons with `id` above. **TODO: insert link to v2 codebook.** |
| `district_ids_in_v2` | A list of identifiers of district polygons defined in the v2 version of this data that covers the year 2022 and onwards that correspond to polygons with `id` above. **TODO: insert link to v2 codebook.** |

## 1.2 `Municipalities` {#municipalities}

`Municipalities` consist of 2205 zones (compared to 8125 municipalities they are based on). Same as above, but without any spatial detalisation in the cities.

To access it:

```{r}
municipalities_v1 <- spod_get_zones("muni", ver = 1)
```

The resulting `municipalities_v1` object is type `sf` consisting of polygons.

Data structure:

| Variable Name | **Description** |
|------------------------------------|------------------------------------|
| `id` | District `id` assigned by the data provider. Matches with `id_origin`, `id_destination`, and `id` in municipality level [origin-destination data](#od-data) and [trips per person data](#tpp-data). |
| `municipalities` | A list of municipality identifiers as classified by the Spanish Statistical Office (INE) that correspond to polygons with `id` above. |
| `districts_mitma` | A list of district identifiers as assigned by the data provider in districts zones spatial dataset that correspond to a given municipality `id` . |
| `census_districts` | A list of census district identifiers as classified by the Spanish Statistical Office (INE) that are spatially bound within polygons with `id` above. |
| `municipality_names_in_v2` | A list of names of municipality polygons defined in the v2 version of this data that covers the year 2022 and onwards that correspond to polygons with `id` above. **TODO: insert link to v2 codebook.** |
| `municipality_ids_in_v2` | A list of identifiers of municipality polygons defined in the v2 version of this data that covers the year 2022 and onwards that correspond to polygons with `id` above. **TODO: insert link to v2 codebook.** |

The spatial data you get via `spanishoddata` package is downloaded directly from the source, the geometries of polygons are automatically fixed if there are any invalid geometries. The zone identifiers are stored in `id` column. Apart from that `id` column, the the original zones files do not have any metadata. However, as seen above, using the `spanishoddata` package you get many additional columns that provide a semantic connection between official statistical zones used by the Spanish government and the zones you can get for the v2 data (for 2022 onward).

# 2. Mobility data

All mobility data is referenced via `id_origin`, `id_destination`, or other location identifiers (mostly labelled as `id`) with the two sets of zones described above.

## 2.1. Origin-destination data {#od-data}

The origin-destination data contain the number of trips between `districts` or `municipalities` in Spain for every hour of every day between 2020-02-14 and 2021-05-09. Each flow also has attributes such as the trip purpose (composed of the type of activity at both the origin and destination), province of residence of individuals making this trip, distance covered while making the trip. See the detailed attributes below in a table. @fig-flows-barcelona shows an example of total flows in the province of Barcelona on Feb 14th, 2020.

![Origin destination flows in Barcelona on 2020-02-14](media/flows_plot.svg){#fig-flows-barcelona width="70%"}

Here are the variables you can find in both the `district` and `municipality` level origin-destination data:

| **English Variable Name** | **Original Variable Name** | Type | **Description** |
|---------------|---------------|---------------|---------------------------|
| `date` | `fecha` | `Date` | The date of the recorded data, formatted as `YYYY-MM-DD`. |
| `id_origin` | `origen` | `factor` | The origin zone `id` of `district` or `municipalitity`. |
| `id_destination` | `destino` | `factor` | The destination zone `id` of `district` or `municipalitity`. |
| `activity_origin` | `actividad_origen` | `factor` | The type of activity at the origin zone, recoded from `casa`, `otros`, `trabajo_estudio` to `home`, `other`, `work_or_study` respectively. |
| `activity_destination` | `actividad_destino` | `factor` | The type of activity at the destination zone, similarly recoded as for `activity_origin` above. |
| `residence_province_ine_code` | `residencia` | `factor` | The province code of residence if individuals who were making the trips in `n_trips`, encoded as province codes as classified by the Spanish Statistical Office (INE). |
| `residence_province_name` | Derived from `residencia` | `factor` | The full name of the residence province, derived from the province code above. |
| `time_slot` | `periodo` | `integer` | The time slot during which the trips occurred. |
| `distance` | `distancia` | `factor` | The distance range of the trip, categorized into specific intervals such as `0005-002` (500 m to 2 km), `002-005` (2-5 km), `005-010` (5-10km), `010-050` (10-50 km), `050-100` (50-100 km), and `100+` (more than 100 km). |
| `n_trips` | `viajes` | `numeric` | The number of trips for that specific origin-destination pair and time slot. |
| `trips_total_length_km` | `viajes_km` | `numeric` | The total length of trips in kilometers, summing up all trips between the origin and destination zones. |
| `year` | `year` | `integer` | The year of the recorded data, extracted from the date. |
| `month` | `month` | `integer` | The month of the recorded data, extracted from the date. |
| `day` | `day` | `integer` | The day of the recorded data, extracted from the date. |

**Data transformation note**

The original data is stored in the [`maestra-1`](https://www.transportes.gob.es/ministerio/proyectos-singulares/estudios-de-movilidad-con-big-data/estudios-de-movilidad-anteriores/covid-19/opendata-movilidad){target="_blank"} folder with suffixes `distritos` (for district zoning) and `municipios` (for municipality zoning). We only use the `district` level data because of several data issues with the `municipality` data documented [here](http://www.ekotov.pro/mitma-data-issues/issues/011-v1-tpp-mismatch-zone-ids-in-table-and-spatial-data.html){target="_blank"} and [here](http://www.ekotov.pro/mitma-data-issues/issues/012-v1-tpp-district-files-in-municipality-folders.html){target="_blank"}, but also because the distric level data contains more columns with useful origin-destination flow characteristics. As a result, you get both the `district` level data and the `municipality` level data with the same columns. `Municipality` level data is simply a re-aggregation of `district` level data using the official relations file where `district` identifiers are mapped to `municipality` identifiers (orginal file is [`relaciones_distrito_mitma.csv`](https://opendata-movilidad.mitma.es/relaciones_distrito_mitma.csv){target="_blank"}).

**Getting the data**

To access the data, use the `spod_get_od()` function. In this example we will use a short interval of dates:

```{r}
dates <- c(start = "2020-02-14", end = "2020-02-17")
od_dist <- spod_get(type = "od", zones = "dist", dates = dates)
od_muni <- spod_get(type = "od", zones = "muni", dates = dates)
```

The data for the specified dates will be automatically downloaded and cached in the `SPANISH_OD_DATA_DIR` directory. Existing files will not be re-downloaded.

**Working with the data**

The resulting objects `od_dist` and `od_muni` are of class `tbl_duckdb_connection`[^1]. Basically, you can treat these as regular `data.frame`s or `tibble`s. One important difference is that the data is not actually loaded into memory, because if you requested more dates, e.g. a whole month or a year, all that data would most likely not fit into your computer's memory. A `tbl_duckdb_connection` is mapped to the downloaded CSV files that are cached on disk and the data is only loaded in small chunks as needed at the time of computation. You can manipulate `od_dist` and `od_muni` using `{dplyr}` functions such as `select()`, `filter()`, `mutate()`, `group_by()`, `summarise()`, etc. In the end of any sequence of commands you will need to add `collect()` to execute the whole chain of data manipulations and load the results into memory in an R `data.frame`/`tibble` like so:

[^1]: For reference: this object also has classes: `tbl_dbi` ,`tbl_sql`, `tbl_lazy` ,and `tbl` .

```{r}
library(dplyr)
od_mean_hourly_trips_over_the_4_days <- od_dist |>
  group_by(time_slot) |>
  summarise(
    mean_hourly_trips = mean(n_trips, na.rm = TRUE),
    .groups = "drop") |> 
  collect()
od_mean_hourly_trips_over_the_4_days
```

```         
# A tibble: 24 × 2
   time_slot mean_hourly_trips
       <int>             <dbl>
 1        18              21.4
 2        10              19.3
 3         2              14.8
 4        15              19.8
 5        11              19.9
 6        16              19.6
 7        22              20.9
 8         0              18.6
 9        13              21.1
10        19              22.5
# ℹ 14 more rows
# ℹ Use `print(n = ...)` to see more rows
```

In this example above, we calculated mean hourly flows over the 4 days of the requested period. The full data for all 4 days was never loaded into memory all at once. Rather the available memory of the computer was used up to its maximum limit to make that calculation happen, without ever exceeding the available memory limit. This is done transparantly to the user with the help of [`DuckDB`](https://duckdb.org/){target="_blank"} (specifically, with [{duckdb} R package](https://r.duckdb.org/index.html){target="_blank"}).

The same summary operation as provided in the example above can be done with the entire dataset for the full 18 month on a regular laptop with 8-16 GB memory. It will take a bit of time to complete, but it will be done. To speed things up, we will also demonstrate in the end of the document, how the data can be converted to more efficient formats.

***Note**: As long as you use object `od_dist` created with `spod_get_od()` function, it is much quicker to filter the dates by the `year`, `month` and `day` variables, rather than by the `date` variable.* This is because the data for each day is in a separate CSV file located in folders that look like `year=2020/month=2/day=14`. So when filtering by the `date` field, R will have to scan all CSV files comparing the specified date with what is stored inside the file. However, if you query by *`year`, `month` and `day` variables, R only needs to check these against the path to each CSV file, which is much quicker. This caveat is only relevant as long as you use `spod_get_od()` . If you convert (see the [relevant section below](#convert-data)) the downloaded data to a format that it optimized for quick analysis, you can use whichever field you want, it should not affect the performance as much.*

## 2.2. Trips per person data {#tpp-data}

The "trips per person" data shows the number of persons by the number of trips they made from each district or municipality.

|  |  |  |  |
|---------------|---------------|---------------|---------------------------|
| `date` | `fecha` | `Date` | The date of the recorded data, formatted as `YYYY-MM-DD`. |
| `id` | `distrito` | `factor` | The identifier of the `district` or `municipality` zone. |
| `n_trips` | `numero_viajes` | `factor` | The number of trips per person, categorized into `0`, `1`, `2`, or `2+` trips. |
| `n_persons` | `personas` | `factor` | The number of persons making the trips from `district` or `municipality` with zone `id`. |
| `year` | `year` | `integer` | The year of the recorded data, extracted from the date. |
| `month` | `month` | `integer` | The month of the recorded data, extracted from the date. |
| `day` | `day` | `integer` | The day of the recorded data, extracted from the date. |


To access it use `spod_get()` with `type` set to "trips_per_persons", or just "tpp". We can also set `dates` to the maximum possible date range `2020-02-14` to `2021-05-09` to get all the data, as this data is relatively small (under 200 Mb).

```{r}
dates <- c(start = "2020-02-14", end = "2021-05-09")
tpp_dist <- spod_get(type = "trips_per_person", zones = "dist", dates = dates)
```

Because this data is small, we can actually load it completely into memory:

```{r}
tpp_dist_tbl <- tpp_dist |> dplyr::collect()
```



# Converting the data for faster analysis {#convert-data}

You can continue working with the tables that you get from `spod_get()` function, but this is slow and only appropriate if you are interested in just a few days of data. To speed up the analysis and to blaze throuth the whole dataset even on a laptop with 8-16 GB of memory, we recommend converting the data into analysis ready formats such as `DuckDB` and `Parquet`.

**Converting the data to either `DuckDB` or `Parquet` will improve the computation time up to 100x compared to continuing working with the tables that you get from `spod_get()` function that reads CSV files every time you do anything.**

`DuckDB` and `Parquet` are cutting edge tools to work with larger than memory datasets such as the mobility data described here. For an introduction to these easy to use tools, we highly recommend the materials by Danielle Navarro, Jonathan Keane, Stephanie Hazlitt: [website](https://arrow-user2022.netlify.app/){target="_blank"}, [slides](https://arrow-user2022.netlify.app/slides){target="_blank"}, and [the video tutorial](https://www.youtube.com/watch?v=YZMuFavEgA4){target="_blank"}.

Learning to use `DuckDB` and `Parquet` is easy for anyone who have ever worked with `{dplyr}` functions such as `select()`, `filter()`, `mutate()`, `group_by()`, `summarise()`, etc. However, since there is some learning curve to master these new tools, we provide some helper functions for novices to get started and easily open the datasets from `DuckDB` and `Parquet`. Please read on. We first show how to convert the data, and then how to use it.

The choice between converting to `DuckDB` and `Parquet` should be made based on your needs:

-   If you plan to download the data for all dates at once and then convert it for analysis, we recommend `DuckDB`, as it is one big file and it is easier to update it completely if for example you were to first use only dates for 2020, and then later decided to add more dates from 2021. In this case it would be better to delete the database and create it from scratch.

-   If you don't plan to download the whole dataset at once and only want certain dates, do analysis on them and then add more dates later, it is better to choose `Parquet` format, as in this case, each day is saved in a separate files, just like the original CSV files. Therefore updating a folder of `Parquet` files is as easy as just creating a new file only for the missing date.

In our testing, using `DuckDB` has a slight speed advantage. The approaches to working with both formats are exactly the same, just use `{dplyr}` package and its functions for selecting, grouping, filtering and summarizing the data.

## 1. `DuckDB` {#duckdb}

### 1.1 Convert to `DuckDB` {#convert-to-duckdb}

You can convert the already downloaded data into `DuckDB` like. For example, you select a few dates, and download the data manually:

```{r}
dates_1 <- c(start = "2020-02-14", end = "2020-02-17")
spod_download_data(type = "od", zones = "distr", dates = dates_1)
```

After that, you can convert any downloaded data (including the files that might have been downloaded previosly by running `spod_get()` or `spod_download_data()` with other dates or date intervals) into `DuckDB` like so:

```{r}
db_1 <- spod_convert_for_analysis(type = "od", zones = "distr", dates = "cached_v1", save_format = "duckdb", overwrite = TRUE)
db_1 # check the path to the saved `DuckDB` database
```

The `dates = "cached_v1"` argument instructs the functon to only work with already downloaded files. By default this resulting `DuckDB` database for v1 origin-destination data for districts will be saved in the `SPANISH_OD_DATA_DIR` directory under `v1/clean_data/tabular/duckdb/` with filename `od_distritos.duckdb`. The function outputs the full path to the database file, which we save into `db_1` variable.

You can also any desired save location with the `save_path` argument of `spod_convert_for_analysis()`.

You can also convert any dates range or dates list to `DuckDB` like so:

```{r}
dates_2 <- c(start = "2020-02-17", end = "2020-02-19")
db_2 <- spod_convert_for_analysis(type = "od", zones = "distr", dates = dates_2, overwrite = TRUE)
```

In this case, any missing data that has now yet been downloaded will be automatically downloaded, while 2020-02-17 will not be redownloaded, as we already requsted it when creating `db_1`. Then the requested dates will be converted into `DuckDB`, overwriting the file with `db_1`. Once again, we save the path to the output `DuckDB` database file into `db_2` variable.

### 1.2 Load the converted `DuckDB` {#load-converted-duckdb}

You can read the introductory information on how to connect to `DuckDB` files [here](https://duckdb.org/docs/api/r){target="_blank"}, however to simplify things for you we created a helper function. So to connect to the data stored in at path `db_1` and `db_2` you can do the following:

```{r}
my_od_data_2 <- spod_connect_to_converted_data(db_2)
```

Just like before, with `spod_get()` funciton that we used to download raw CSV.gz files and analyse them without any conversion, the resulting object `my_od_data_2` is also a `tbl_duckdb_connection`. So, you can treat it as regular `data.frame` or `tibble` and use `{dplyr}` functions such as `select()`, `filter()`, `mutate()`, `group_by()`, `summarise()`, etc.

After finishing working with `my_od_data_2` we advise that you "disconnect" this data using:

```{r}
spod_disconnect_data(my_od_data_2)
```

This is usefyl to free up memory. This is espetially neccessary if you convert the data with `spod_convert_for_analysis()` and then load it with `spod_connect_to_converted_data()` again, and then would like to run `spod_convert_for_analysis()` again and save the data to the same location. Otherwise, it is also helpful to avoid unnecessary possible warnings in terminal for garbage collected connections.


## 2. `Parquet` {#arrow-parquet}

The process is exactly the same as for `DuckDB` above. The only difference is that the data is converted to `parquet` format and stored in `SPANISH_OD_DATA_DIR` under `v1/clean_data/tabular/parquet/` directory (by default, customizesable by the `save_path` argument), and the subfolders are in hive-style format like `year=2020/month=2/day=14` and inside each of these folders a single `parquet` file will be placed containing the data for that day.

The advantage of this format is that you can "update" it quickly. For example, if you first downloaded the data for March and April 2020, converted this period into `parquet` format, and then downloaded the data for May and June 2020, when you run the convertion function again, it will only convert the data for May and June 2020 and add it to the existing `parquet` files. So you will save time and will not have to wait for March and April 2020 to be converted again.

### 2.1 Convert to `Parquet` {#convert-to-parquet}

Let us convert a few dates to `parquet` format:

```{r}
type <- "od"
zones <- "distr"
dates <- c(start = "2020-02-14", end = "2020-02-17")
od_distr_feb14_feb16_2020 <- spod_convert_for_analysis(type = type, zones = zones, dates = dates, save_format = "parquet")
```


If we now request additional dates that overlap with the already converted data like so and specifiy argument `overwrite = 'update'` we will update the existing `parquet` files with the new data:

```{r}
dates <- c(start = "2020-02-16", end = "2020-02-19")
od_distr_feb16_feb19_2020 <- spod_convert_for_analysis(type = type, zones = zones, dates = dates, save_format = "parquet", overwrite = 'update')
```

That is, 16 and 17 Feboruary will not be converted again. Only the new data, that was not converted (18 and 19 February) will be converted, and these will be added to the existing folder structure of`parquet` files stored at the default `save_path` location, which is `<data_dir>/clean_data/v1/tabular/parquet/od_distritos`. Alternatively, you can set any other save location by setting the `save_path` argument.

### 2.2 Load the converted `Parquet` {#load-converted-parquet}

Working with these `parquet` files is exactly the same as with `DuckDB` and `Arrow` files. Just like before, you can use the same helper function `spod_connect_to_converted_data()` to connect to the `parquet` files:

```{r}
my_od_data_3 <- spod_connect_to_converted_data(od_distr_feb16_feb19_2020)
```

Mind you though, because we have first converted the data for a period between 14 and 17 February 2020, and then converted the data for a period between 16 and 19 February 2020 into the save default location, the `od_distr_feb16_feb19_2020` contains the path to all this data, and therefore `my_od_data_3` will connect you to all data.

You can check this like so:

```{r}
my_od_data_3 |> 
  dplyr::distinct(date) |>
  dplyr::arrange(date)
```
